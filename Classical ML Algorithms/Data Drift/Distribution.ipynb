{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJUk+AbkINe8yOdIb2gUMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgreat5/ML/blob/main/Classical%20ML%20Algorithms/Data%20Drift/Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ways to check distribution difference between Train and Test**\n"
      ],
      "metadata": {
        "id": "lZE-P0Cvw8QD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Adversial Validation**\n",
        "\n",
        "*To check if the Train data and Test data follow the same distribution, follow the below approach:*\n",
        "\n",
        "*   Mix all the data, remove the target variable, and add a new binary target that contained a value of 1 for each training sample and a value of 0 for each test sample.\n",
        "*   Train a binary classification model using the above created dataset to see if samples can be separated based on whether they belonged to Training data or Test data.\n",
        "*   Using Area under the ROC curve, we can evaluate the results. \n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VdCGeeuNxHNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZjXHs2gwlXc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC (Receiver Operating Characteristic) Curve** to evaluate the results.It is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
        "*True Positive Rate* and *False Positive Rate*\n",
        "\n",
        "This metric measures the area underneath the ROC curve, which ranges between 0.0 and 1.0.\n",
        "\n",
        "*   The more accurate the model results are, the higher the area under the curve will be. Therefore, if the model can tell the training and test samples apart, the closer the AUC will be to 1.0.\n",
        "*   Conversely, the closer the model performs to random chance, the worse it is at telling training samples apart from test samples. In this case, we will see the AUC close to 0.5, which means that both sets come from the same distribution."
      ],
      "metadata": {
        "id": "8onZfMBDyd4D"
      }
    }
  ]
}